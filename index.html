<!DOCTYPE HTML>

<html>
	<head>
		<title>Man vs AI</title>
		<meta charset="utf-8" />
		<meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no" />
		<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@4.0.0/dist/css/bootstrap.min.css" integrity="sha384-Gn5384xqQ1aoWXA+058RXPxPg6fy4IWvTNh0E263XmFcJlSAwiGgFAW/dAiS6JXm" crossorigin="anonymous">
		<link rel="stylesheet" href="assets/css/main.css" />
		<link rel="stylesheet" href="assets/css/custom.css" />
		<noscript><link rel="stylesheet" href="assets/css/noscript.css" /></noscript>
	</head>
	<body class="is-preload">

			<!-- Header -->
			<header id="header">
				<a id="logo" class="logo" href="#">Klech</a>
				<nav>
					<ul>
						<li><a href="#home">Home</a></li>
						<li><a href="#intro">Introduction</a></li>
						<li><a href="#questions">Research Questions</a></li>
						<li><a href="#datastory">Datastory</a></li>
						<li><a href="#footer">Team</a></li>
					</ul>
				</nav>
			</header>

			<!-- Home -->
			<section id="home" class="main style1 dark fullscreen" tabindex="">
				<div class="content">
					<header>
						<h2>Man vs AI: A Comparison of Human and LLM Wikispeedia Strategy</h2>
					</header>
					<p id="anim"></p>
				</div>
			</section>

			<!-- Introduction -->
			<section id="intro" class="main style2 right dark fullscreen">
				<div class="content box style2">
					<header>
						<h2>Introduction</h2>
					</header>
					<p>
						LLMs are trained on extremely large corpuses of texts, most of the time written by humans over decades. This enables them to generate human-like sentences and reply to questions in a sound way. But how closely do they adopt human ways of thought? More specifically, can an LLM emulate through behavior the same thinking underpinning human semantic maps? 
					</p> <p>
						To answer this question, we will enlist ChatGPT as a participant in Wikispeedia and evaluate its performance across a subset of popular but meaningfully diverse origin-goal page pairs previously played by humans. 
						Our analysis will first parse ChatGPT's decisions for human 'readability' - that is, we will employ embeddings, TF-IDF vectorization on page content, and Wikispeedia-derived human semantic distances to determine if we, as humans, can justify ChatGPT's chosen paths. 
						We will then will compare ChatGPT's paths to human paths, measuring levels of similarity in rounds-to-goal, 'zoom-in' / 'zoom-out', and rates of 'course correction', thereby quantifying ChatGPT's proximity to human strategization and ex-ante semantic mapping. 
					</p>
				</div>
				<a href="#questions" class="button style2 down anchored">Next</a>
			</section>

			<!-- Research Questions -->
			<section id="questions" class="main style2 left dark fullscreen">
				<div class="content box style2">
					<header>
						<h2>Research Questions</h2>
					</header>
					<ul id="questions-list">
						<li>Does ChatGPT pursue page-paths that are sensical to human researchers?</li>
						<li>Is the previous question's answer robust to different semantic categories?</li>
						<li>How does ChatGPT's rounds-to-goal compare to human players'?</li>
						<li>Does ChatGPT employ the same 'zoom-out' to hub, 'zoom-in' to spoke Wikispeedia strategy as humans?</li>
						<li>How often does ChatGPT 'backtrack' compared to humans?</li>
					</ul>
				</div>
				<a href="#datastory" class="button style2 down anchored">Next</a>
			</section>

			<!-- Datastory -->
			<h2 id="datastory" style="text-align: center; margin-top: 50px">Datastory</h2>
			
			<section class="main style3 primary">
				<div class="content">
					<header>
						<h3>Overview</h3>
						<p>Some words on the total number of games polayed, total finished total unfinished. type of data (graph), goal of wikispeedia. Very few words</p>
					</header>

					<div>
						<div class="flourish-embed flourish-chart" data-src="visualisation/16169106"><script src="https://public.flourish.studio/resources/embed.js"></script></div>
						<p> comments on the plot above <p>

						<div class="flourish-embed flourish-chart" data-src="visualisation/16168925"><script src="https://public.flourish.studio/resources/embed.js"></script></div>
						<p> comments on the plot above <p>
						
						<div class="flourish-embed flourish-sankey" data-src="visualisation/16169537"><script src="https://public.flourish.studio/resources/embed.js"></script></div>
						<p> comments on the plot above <p>
						
						<div class="flourish-embed flourish-sankey" data-src="visualisation/16179089"><script src="https://public.flourish.studio/resources/embed.js"></script></div>
						<p> comments on the plot above <p>
						
						<p> Final words on this section, introduction to the next. </p>
					</div>

				</div>
			</section>

			<section class="gray main style3">
				<div class="content">
					<header>
						<h3>Choice of LLM</h3>
						<p>
						Undertaking this project meant deciding which LLM we would ask to play Wikispeedia. This is not a simple choice. On the one hand, we needed a model trained on enough parameters to be able to consistently react appropriately to a series of Wikispeedia prompts, else we would not be able to programmatically generate AI-completed game paths.
						OpenAI’s ChatGPT fits the bill. On the other hand, we’ve neither funding nor research-grade computational power, precluding use of OpenAI’s API at scale. Our economic and computational restrictions encouraged us to limit the scope of LLM candidates to those which are (1) publicly available on the HuggingFace repository and (2) trained with 13B
						parameters or less. After consulting the HuggingFace <a href="https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard">[leaderboard]</a>, we focused on the following two models:<br>
							
							<ul>
								<li><span class="bold">LlaMA-13B</span> : see the model on <a href="https://huggingface.co/meta-llama/Llama-2-13b-hf">[hugging-face]</a></li>
								<li><span class="bold">Mistral-7B</span> : see the model on <a href="https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.2">[hugging-face]</a></li>
							</ul>
							
							Despite LlaMA-13B’s parameter advantage, Mistral-7B could more consistently respond appropriately to our prompts (see prompt details below). This was the case in particular each model’s <span class="italic">instruct</span> version, which refers to the model variant trained to follow instructions. Mistral’s relative success is in line with research outcomes as well, such as the following set of benchmarks from <span class="italic">Jiang, Albert Q., et al. 2023 </span> <a href="https://arxiv.org/pdf/2310.06825.pdf">[arXiv]:
						</p>
						<img src="./assets/images/analysis_imgs/MISTRALvsLLaMA_hist.png" width="100%" height="auto" alt="Mistral vs LLaMA hist">
						<img src="./assets/images/analysis_imgs/MISTRALvsLLaMA_table.png" width="100%" height="auto" alt="Mistral vs LLaMA table">
						
							
					</header>

					<div>
						<p> Once the model was chosen, we faced and solved two other roadblocks:
							<ol type="1">
								<li> 1. Finding a good prompt: we needed to find a prompt architecture that will allow us to emulate the Wikispeedia game experience. Large language models have demonstrated outstanding performance on a wide range of tasks 
									such as question answering. On a high level, given an input, we want the language model to automatically reply by following instructions related to our downstream task. However, language models are highly dependent on the choice of prompts, and usually 
									small changes in the questions results in very different answers form the model. This is especially true when dealing with small models, like in our case. Afer literature review and several QA iterations on the 
									model, we opted for the following chain-of-tought with one shot learning: <br>

									<span class="italic"> We now play the following game: <br>
										
									I will give you a target word and a list from which you can choose an option. If the list contains the target word, you choose it. Otherwise you choose the option that is most similar to it. Before starting, I give you one examples, then it's your turn: <>
                
									EXAMPLE:<br>
									Target word: George_Washington<br>
									Available options: [[Able_Archer_83, Afghanistan, , Estonia, Europe, Finland, France, French_language, George_W._Bush, Hungary,   September_11,_2001_attacks, United_States]] <br>
									Reasoning: I need to find something inside the list related to the target: 'George_Washington'. George Washington was the first president of United States and he lived in United States. <br>
									Answer: Hence the answer is: 'United_States'. <br>

									YOUR TURN: <br>
									Target word: {TARGET} <br>
									Available options: [[{LIST OF LINKS}]] <br>
									Reasoning: [REASONING] <br>
									Answer: Hence the choice is: '[ANSWER]'</span> <br>

								</li>
								<li> 2. Let the (relatively small) model follow instrucitons: despite the elaborated and tested prompt, the model was still sometime hallucinating 
									answers outside the provided list of links. To solve this, we used the <a href="https://lmql.ai/">LMQL</a> library (SRIlab @ETH Zürich) which
									easily allows to specify constraints on the language model output, like <span class="italic">Choice From Set</span>. For more information on this library, we suggest the following 
									set of documents <a href="https://lmql.ai/docs/">[docs]</a>  which is all-encompassing and easy to follow.</li><br>
							</ol>
						</p>
						<p>
						Now that we have the prompt and we know ghow to let it follow the instructions, we created a pipeline that let the model play all the games that huan players tried (completed or not) more than 
						20 times, resulting in 75 games in total. Each game was attempted by Mistral a total nuymber of 30 trials per game. Which brings us to the following paragraph. 
						</p>
					<div>
				</div>
			</section>

			<section class="main style3 primary">
				<div class="content">
					<header>
						<h3>Human vs Mistral: Who is better at Wikispeedia?</h3>
						<p>Our first route of investigation stems from the rather natural and universal question of whether AI can outperform humans in tasks requiring critical thought. To answer this in the realm of Wikispeedia, we first process Mistral’s prompt responses so they conform to the structure of preexisting, human-derived Wikispeedia data. We then (1) drop all games where the human player quit or the Mistral game was aborted due to our 20-page maximum permittance (which we established to reduce computation time) and (2) calculate average human game length and average Mistral game length for each game pair. These averages are plotted below alongside 95% confidence intervals; those game pair labels rendered bold exhibit a statistically significant difference in average game length between humans and Mistral. Finally, note that game pairs are sorted by the difference in average game length, meaning the higher the game pair is in the chart, the more the average human outperforms Mistral..</p>
					</header>

					<div>
						<img src="./assets/images/analysis_imgs/human_vs_mistral_kaede.png" width="100%" height="auto" alt="Human vs Mistral">
						<p>Those who spend unpaid manhours clicking through Wikipedia links for fun need not worry: the balance of game length – or speed - appears to be in humans’ favor. Across the 75 game pairs we had Mistral play (but not necessarily finish) 30 times each, just 5 show Mistral beating humans’ average speed with statistical significance. Humans, meanwhile, trump Mistral in 29 game pairs – nearly 40% of the set. What’s more, the scale of humanity’s wins is often larger than Mistral’s, as humanity can claim the top 19 highest-magnitude speed differentials. This is all to say that if you select a random game pair, humans are more likely than Mistral to be speedier at it, and when humans win, they win big..</p>
						<p>We formalize our finding that humans are generally the speedier Wikispeedia player on a global scale through a t-test on game length across matched observations. If HF<sub>i</sub> is the number of finished human game paths for game pair i, and MF<sub>i</sub> is the number of finished Mistral game paths for game pair i, we sample min(HF<sub>i</sub>,MF<sub>i</sub>) observations from both human and Mistral data for each game pair i, match human observations to Mistral observations using game pair, take the difference in each pair’s game length, and perform a t-test for difference from 0 on the global set of these differences. We find humans use approximately one page less to reach their target in the matched set (this result is significant at the 99% confidence level). </p>
						<p>EXACT RESULT IN CASE WE WANT A GRAPHIC OR CARD: t-statistic: -8.777983246249146, p-value: 3.8352417389369964e-18, avg: -0.9988726042841037</p>

						<p>We should note that speed is not the only metric for measuring performance. Completion rate is an important metric as well. After all, what good is a talented athlete if he or she quits halfway through every game? Below, we report the cumulative proportion of games completed over time by both Mistral and humans in the top 5 most-played games without a clear speed-based winner. Above, these games were a wash. Notice now how completion rates paint a different picture:</p>
						<img src="./assets/images/analysis_imgs/cumulative_completed.png" width="100%" height="auto" alt="Cumulative Completed">
						<p>Clearly, Mistral has greater follow through in these game pairs than humans do. Perhaps drawn toward certain game pairs by perceived ease (Brain -> Telephone) or interesting pages (Cat->Computer, because who doesn’t love cats?) only to realize the game pair is more cumbersome than initially believed, humans appear prone to abandon attempts. Mistral, meanwhile, doesn’t have this freedom under our current prompt; it can only ‘abandon’ a game when it has made too many traversals or engaged in too many cycles. That it isn’t forced to abandon more games under these conditions suggests it is perceived difficulty rather than actual difficulty (which would take the form of exclusively non-semantically-intuitive paths toward the goal) that encourages humans to quit. Mistral is immune to the demoralizing effect of this perception, and is stronger player for it.
							Yet the reason behind differences in completion rates is immediately intuitive. Less so the reason behind differences in speed. We thus turn our focus toward understanding <em>why</em> humans tend to be faster than Mistral among completed game paths. 
							</p>
					</div>
				</div>
			</section>

			<section class="gray main style3">
				<div class="content">
					<header>
						<h3>Why?</h3>
						<p>introduction to investigation methods:
							- heatmaps
							- zoom out zoom in 
							- semantic distance step to step + step to goal 
							- how sensical to human (overlap between paths)
						</p>
					</header>

					<div>
						<div class="img-pair">
							<img src="./assets/images/analysis_imgs/heatmap_human.png" width="70%" height="auto" alt="Heatmap human">
							<img src="./assets/images/analysis_imgs/heatmap_mistral.png" width="70%" height="auto" alt="Heatmap mistral">
						</div>
						<p>Comments on the plot above </p>
					</div>
				</div>
			</section>

			<section class="main style3 primary">
				<div class="content">
					<header>
						<h3>Conclusions</h3>
						<img src="./assets/images/ai-vs-human-artificial-intelligence.png" width="100%" height="auto" alt="Mistral vs LLaMA hist">
						<p>Write some conclusions</p>
					</header>

				</div>
			</section>

			<!-- Footer -->
			<footer id="footer">
					<section id="contact-section" class="features contact"> 
						<div class="contact-container"> 
							<div class="row m-b-10" style="margin-bottom: 4%;">
								<div class="col-lg-12 text-center">
									 <div class="navy-line"></div> 
									 <h1 style="font-size: 150%;">Team</h1>
							</div>
							</div>
							<div class="row justify-content-center">
								<div class="col-sm-2">
									<div class="team-member">
										<h4><span class="navy">Ernesto</span> Bocini</h4>
										<p>______</p>
										<ul class="list-inline social-icon"> 
											<li><a href="https://github.com/ernestoBocini" target="blank"><i class="icon brands fa-github"></i></a> </li>
											<li><a href="mailto:ernesto.bocini@epfl.ch"><i class="fa fa-envelope"></i></a></li>
										</ul>
									</div>
								</div>
								<div class="col-sm-2">
									<div class="team-member">
										<h4><span class="navy">Lorenzo</span> Drudi</h4>
										<p>______</p>
										<ul class="list-inline social-icon">
											<li><a href="https://github.com/drudilorenzo" target="blank"><i class="icon brands fa-github"></i></a> </li>
											<li><a href="mailto:lorenzo.drudi@epfl.ch"><i class="fa fa-envelope"></i></a></li>
										</ul>
									</div>
								</div>
								<div class="col-sm-2">
									<div class="team-member">
										<h4><span class="navy">Kaede</span> Johnson</h4>
										<p>______</p>
										<ul class="list-inline social-icon">
											<li><a href="https://github.com/kaedejohnson" target="blank"><i class="icon brands fa-github"></i></a> </li>
											<li><a href="mailto:kaede.johnson@epfl.ch"><i class="fa fa-envelope"></i></a> </li>
										 </ul>
									</div>
								</div>
								<div class="col-sm-2">
									<div class="team-member">
										<h4><span class="navy">Hanwen</span> Zhang</h4>
										<p>______</p>
										<ul class="list-inline social-icon">
											<li><a href="https://github.com/Katie-zhang" target="blank"><i class="icon brands fa-github"></i></a> </li>
											<li><a href="mailto:hanwen.zhang@epfl.ch"><i class="fa fa-envelope"></i></a> </li>
										</ul>
									</div>
								</div>
								<div class="col-sm-2">
									<div class="team-member">
										<h4><span class="navy">Xingyue</span> Zhang</h4>
										<p>______</p>
										<ul class="list-inline social-icon">
											<li><a href="https://github.com/bREAKtHEdOLL" target="blank"><i class="icon brands fa-github"></i></a> </li>
											<li><a href="mailto:Xingyue.zhang@epfl.ch"><i class="fa fa-envelope"></i></a> </li>
										</ul>
									</div>
								</div>
							</div>

					<ul id="repo-link" class="menu">
						<li><a href="https://github.com/epfl-ada/ada-2023-project-klech" class="icon brands fa-github"><span class="label">Github</span></a></li>
						<li >&copy; KlechxADA</li></li>
					</ul>

			</footer>

			<!-- Scripts -->
			<script src="assets/js/jquery.min.js"></script>
			<script src="assets/js/jquery.poptrox.min.js"></script>
			<script src="assets/js/jquery.scrolly.min.js"></script>
			<script src="assets/js/jquery.scrollex.min.js"></script>
			<script src="assets/js/browser.min.js"></script>
			<script src="assets/js/breakpoints.min.js"></script>
			<script src="assets/js/util.js"></script>
			<script src="assets/js/main.js"></script>
			<script src="assets/js/typing.js"></script>

			<script src="https://code.jquery.com/jquery-3.2.1.slim.min.js" integrity="sha384-KJ3o2DKtIkvYIK3UENzmM7KCkRr/rE9/Qpg6aAZGJwFDMVNA/GpGFF93hXpG5KkN" crossorigin="anonymous"></script>
			<script src="https://cdn.jsdelivr.net/npm/popper.js@1.12.9/dist/umd/popper.min.js" integrity="sha384-ApNbgh9B+Y1QKtv3Rn7W3mgPxhU9K/ScQsAP7hUibX39j7fakFPskvXusvfa0b4Q" crossorigin="anonymous"></script>
			<script src="https://cdn.jsdelivr.net/npm/bootstrap@4.0.0/dist/js/bootstrap.min.js" integrity="sha384-JZR6Spejh4U02d8jOt6vLEHfe/JQGiRRSQQxSfFWpi1MquVdAyjUar5+76PVCmYl" crossorigin="anonymous"></script>
	</body>
</html>