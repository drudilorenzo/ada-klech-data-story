<!DOCTYPE HTML>

<html>
	<head>
		<title>Man vs AI</title>
		<meta charset="utf-8" />
		<meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no" />
		<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@4.0.0/dist/css/bootstrap.min.css" integrity="sha384-Gn5384xqQ1aoWXA+058RXPxPg6fy4IWvTNh0E263XmFcJlSAwiGgFAW/dAiS6JXm" crossorigin="anonymous">
		<link rel="stylesheet" href="assets/css/main.css" />
		<link rel="stylesheet" href="assets/css/custom.css" />
		<noscript><link rel="stylesheet" href="assets/css/noscript.css" /></noscript>
	</head>
	<body class="is-preload">

			<!-- Header -->
			<header id="header">
				<a id="logo" class="logo" href="#">Klech</a>
				<nav>
					<ul>
						<li><a href="#home">Home</a></li>
						<li><a href="#intro">Introduction</a></li>
						<li><a href="#questions">Research Questions</a></li>
						<li><a href="#datastory">Datastory</a></li>
						<li><a href="#footer">Team</a></li>
					</ul>
				</nav>
			</header>

			<!-- Home -->
			<section id="home" class="main style1 dark fullscreen" tabindex="">
				<div class="content">
					<header>
						<h2>Man vs AI: A Comparison of Human and LLM Wikispeedia Performance</h2>
					</header>
					<p id="anim"></p>
				</div>
			</section>

			<!-- Introduction -->
			<section id="intro" class="main style2 right dark fullscreen">
				<div class="content box style2">
					<header>
						<h2>Introduction</h2>
					</header>
					<p>
						LLMs are trained on extremely large corpuses of texts, most of the time written by humans over decades. This enables them to generate human-like sentences and reply to questions in a sound way. But how closely do they adopt human ways of thought? Would an LLM play a game involving semantic relationships the same way humans do? Would they play it better?
					</p> <p>
						To answer these questions, we will enlist an LLM as a participant in Wikispeedia, a game wherein players navigate from one Wikipedia page to another using as few clicks on Wikipedia page links as possible. We then compare the LLM’s results to human results, considering in particular speed, completion rates, progression towards goal over time, location of clicks on the Wikipedia page, and strategic use of Wikipedia pages with many links. Our findings contribute to a societal question more pressing in the past year than perhaps any time in human history: (when) will AI overtake humans’ ability to think creatively?
					</p>
				</div>
				<a href="#questions" class="button style2 down anchored">Next</a>
			</section>

			<!-- Research Questions -->
			<section id="questions" class="main style2 left dark fullscreen">
				<div class="content box style2">
					<header>
						<h2>Research Questions</h2>
					</header>
					<ul id="questions-list">
						<li>Who is better at Wikispeedia: Humanity or an LLM?</li>
						<li>If there is a clear winner, what strategy or phenomenon is producing advantage? </li>
					</ul>
				</div>
				<a href="#datastory" class="button style2 down anchored">Next</a>
			</section>

			<!-- Datastory -->
			<h2 id="datastory" style="text-align: center; margin-top: 50px">Datastory</h2>
			
			<section class="main style3 primary">
				<div class="content">
					<header>
						<h3>Overview</h3>
						<p>Some words on the total number of games polayed, total finished total unfinished. type of data (graph), goal of wikispeedia. Very few words</p>
					</header>

					<div>
						<div class="flourish-embed flourish-chart" data-src="visualisation/16169106"><script src="https://public.flourish.studio/resources/embed.js"></script></div>
						<p> comments on the plot above <p>

						<div class="flourish-embed flourish-chart" data-src="visualisation/16168925"><script src="https://public.flourish.studio/resources/embed.js"></script></div>
						<p> comments on the plot above <p>
						
						<div class="flourish-embed flourish-sankey" data-src="visualisation/16169537"><script src="https://public.flourish.studio/resources/embed.js"></script></div>
						<p> comments on the plot above <p>
						
						<div class="flourish-embed flourish-sankey" data-src="visualisation/16179089"><script src="https://public.flourish.studio/resources/embed.js"></script></div>
						<p> comments on the plot above <p>
						
						<p> Final words on this section, introduction to the next. </p>
					</div>

				</div>
			</section>

			<section class="gray main style3">
				<div class="content">
					<header>
						<h3>Choice of LLM</h3>
						<p>
						Undertaking this project meant deciding which LLM we would ask to play Wikispeedia. This is not a simple choice. On the one hand, we needed a model trained on enough parameters to be able to consistently react appropriately to a series of Wikispeedia prompts, else we would not be able to programmatically generate AI-completed game paths.
						OpenAI’s ChatGPT fits the bill. On the other hand, we’ve neither funding nor research-grade computational power, precluding use of OpenAI’s API at scale. Our economic and computational restrictions encouraged us to limit the scope of LLM candidates to those which are (1) publicly available on the HuggingFace repository and (2) trained with 13B
						parameters or less. After consulting the HuggingFace <a href="https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard">[leaderboard]</a>, we focused on the following two models:<br>
							
							<ul>
								<li><span class="bold">LlaMA-13B</span> : see the model on <a href="https://huggingface.co/meta-llama/Llama-2-13b-hf">[hugging-face]</a></li>
								<li><span class="bold">Mistral-7B</span> : see the model on <a href="https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.2">[hugging-face]</a></li>
							</ul>
							
							Despite LlaMA-13B’s parameter advantage, Mistral-7B could more consistently respond appropriately to our prompts (see prompt details below). This was the case in particular fpr each model’s <span class="italic">instruct</span> version, which refers to the model variant trained to follow instructions. Mistral’s relative success is in line with research outcomes as well, such as the following set of benchmarks from <span class="italic">Jiang, Albert Q., et al. 2023 </span> <a href="https://arxiv.org/pdf/2310.06825.pdf">[arXiv]</a>:
						</p>
						<img src="./assets/images/analysis_imgs/MISTRALvsLLaMA_hist.png" width="100%" height="auto" alt="Mistral vs LLaMA hist">
						<img src="./assets/images/analysis_imgs/MISTRALvsLLaMA_table.png" width="100%" height="auto" alt="Mistral vs LLaMA table">
						
							
					</header>

					<div>
						<p> But successful AI-generated Wikispeedia data depends on our prompt as well as our model. We spent considerable time developing a prompt architecture that emulates the Wikispeedia game experience as a result. 
							<br><br>
							In general, we find that small changes in our prompt yielded different answers and game paths for a given game pair. This is because even if large language models can demonstrate outstanding performance on question-answering tasks, both limited size and sequential responses can compromise this ability. Following literature review and several QA rounds with prompt variants, we opted for the following chain-of-thought prompt architecture with one shot learning:
								<br>
									<div id="prompt" class="italic"> We now play the following game: <br>
										
									I will give you a target word and a list from which you can choose an option. If the list contains the target word, you choose it. Otherwise you choose the option that is most similar to it. Before starting, I give you one examples, then it's your turn: <br>
									EXAMPLE:<br>
									Target word: George_Washington<br>
									Available options: [[Able_Archer_83, Afghanistan, , Estonia, Europe, Finland, France, French_language, George_W._Bush, Hungary,   September_11,_2001_attacks, United_States]] <br>
									Reasoning: I need to find something inside the list related to the target: 'George_Washington'. George Washington was the first president of United States and he lived in United States. <br>
									Answer: Hence the answer is: 'United_States'. <br>

									YOUR TURN: <br>
									Target word: {TARGET} <br>
									Available options: [[{LIST OF LINKS}]] <br>
									Reasoning: [REASONING] <br>
									Answer: Hence the choice is: '[ANSWER]'</div> <br>

									Note that in the prompt above, TARGET refers to the goal of a Wikispeedia game pair and LIST OF LINKS refers to the links available on a given Wikipedia page. LIST OF LINKS changes in each round of the game; TARGET does not.
									<br><br>
									To be clear, our prompt does not fully emulate the options available to human Wikispeedia players. In particular, Mistral is not given the option to revert to previous pages, nor is it given the option to abandon a game. We also maintain a blacklist in each game to ensure Mistral does not choose any link more than twice. Our testing determined that these compromises were necessary in order for Mistral to make continued responses that respected the bounds of our game. 
									<br><br>
									In fact, despite elaborate prompt testing and tuning, we still found that Mistral would occasionally respond with answers not available in the list of links. To solve this, we use the <a href="https://lmql.ai/">LMQL</a> developed by ETH Zürich’s SRIlab lab, which allows us to specify constraints on Mistral’s output – namely, that it choose an option from our provided set of links. For more information on this library, we suggest the following set of documents <a href="https://lmql.ai/docs/">[docs]</a> which is all-encompassing and easy to follow.<br>
						</p>
						<p>
							With our LLM and prompt in place, we launched a pipeline that asked Mistral to play all the Wikispeedia game pairs which human players tried 20 or more times. At 30 rounds each for 75 game pairs, we create 2,250 AI-generated game paths total for our novel dataset. Note that not all of these games were completed, as we aborted any games in which Mistral had failed to reach the goal by round 30.
							<br><br>
							Now for the part we've all been waiting for: comparing Mistral data to human data!
						</p>
					<div>
				</div>
			</section>

			<section class="main style3 primary">
				<div class="content">
					<header>
						<h3>Human vs Mistral: Who is better at Wikispeedia?</h3>
						<p>Our first route of investigation stems from the rather natural and universal question of whether AI can outperform humans in tasks requiring critical thought. To answer this in the realm of Wikispeedia, we first process Mistral’s prompt responses so they conform to the structure of preexisting, human-derived Wikispeedia data. We then (1) drop all games where the human player quit or the Mistral game was aborted due to our 20-page maximum permittance (which we established to reduce computation time) and (2) calculate average human game length and average Mistral game length for each game pair. These averages are plotted below alongside 95% confidence intervals; those game pair labels rendered bold exhibit a statistically significant difference in average game length between humans and Mistral. Finally, note that game pairs are sorted by the difference in average game length, meaning the higher the game pair is in the chart, the more the average human outperforms Mistral..</p>
					</header>

					<div>
						<img src="./assets/images/analysis_imgs/human_vs_mistral_kaede.png" width="100%" height="auto" alt="Human vs Mistral">
						<p>Those who spend unpaid manhours clicking through Wikipedia links for fun need not worry: the balance of game length – or speed - appears to be in humans’ favor. Across the 75 game pairs we had Mistral play (but not necessarily finish) 30 times each, just 5 show Mistral beating humans’ average speed with statistical significance. Humans, meanwhile, trump Mistral in 29 game pairs – nearly 40% of the set. What’s more, the scale of humanity’s wins is often larger than Mistral’s, as humanity can claim the top 19 highest-magnitude speed differentials. This is all to say that if you select a random game pair, humans are more likely than Mistral to be speedier at it, and when humans win, they win big..</p>
						<p>We formalize our finding that humans are generally the speedier Wikispeedia player on a global scale through a t-test on game length across matched observations. If HF<sub>i</sub> is the number of finished human game paths for game pair i, and MF<sub>i</sub> is the number of finished Mistral game paths for game pair i, we sample min(HF<sub>i</sub>,MF<sub>i</sub>) observations from both human and Mistral data for each game pair i, match human observations to Mistral observations using game pair, take the difference in each pair’s game length, and perform a t-test for difference from 0 on the global set of these differences. We find humans use approximately one page less to reach their target in the matched set (this result is significant at the 99% confidence level). </p>
						<p>EXACT RESULT IN CASE WE WANT A GRAPHIC OR CARD: t-statistic: -8.777983246249146, p-value: 3.8352417389369964e-18, avg: -0.9988726042841037</p>

						<p>We should note that speed is not the only metric for measuring performance. Completion rate is an important metric as well. After all, what good is a talented athlete if he or she quits halfway through every game? Below, we report the cumulative proportion of games completed over time by both Mistral and humans in the top 5 most-played games without a clear speed-based winner. Above, these games were a wash. Notice now how completion rates paint a different picture:</p>
						<img src="./assets/images/analysis_imgs/cumulative_completed.png" width="100%" height="auto" alt="Cumulative Completed">
						<p>Clearly, Mistral has greater follow through in these game pairs than humans do. Perhaps drawn toward certain game pairs by perceived ease (Brain -> Telephone) or interesting pages (Cat->Computer, because who doesn’t love cats?) only to realize the game pair is more cumbersome than initially believed, humans appear prone to abandon attempts. Mistral, meanwhile, doesn’t have this freedom under our current prompt; it can only ‘abandon’ a game when it has made too many traversals or engaged in too many cycles. That it isn’t forced to abandon more games under these conditions suggests it is perceived difficulty rather than actual difficulty (which would take the form of exclusively non-semantically-intuitive paths toward the goal) that encourages humans to quit. Mistral is immune to the demoralizing effect of this perception, and is stronger player for it.
							<br><br>The reason behind differences in completion rates is immediately intuitive. Less so the reason behind differences in speed. We thus turn our focus toward understanding <em>why</em> humans tend to be faster than Mistral among completed game paths. 
							</p>
					</div>
				</div>
			</section>

			<section class="gray main style3">
				<div class="content">
					<header>
						<h3>Why?</h3>
						<p>introduction to investigation methods:
							- heatmaps
							- zoom out zoom in 
							- semantic distance step to step + step to goal 
							- how sensical to human (overlap between paths)
						</p>
					</header>

					<div>
						<div class="img-pair">
							<img src="./assets/images/analysis_imgs/heatmap_human.png" width="70%" height="auto" alt="Heatmap human">
							<img src="./assets/images/analysis_imgs/heatmap_mistral.png" width="70%" height="auto" alt="Heatmap mistral">
						</div>
						<p>Comments on the plot above </p>
					</div>
				</div>
			</section>

			<section class="main style3 primary">
				<div class="content">
					<header>
						<h3>Conclusions</h3>
						<img src="./assets/images/ai-vs-human-artificial-intelligence.png" width="100%" height="auto" alt="Mistral vs LLaMA hist">
						<p>Humans can generally complete Wikispeedia faster than Mistral.<br>
						   Mistral will complete Wikispeedia games humans won't, and in a reasonable amount of time as well.

						</p>
					</header>

				</div>
			</section>

			<!-- Footer -->
			<footer id="footer">
					<section id="contact-section" class="features contact"> 
						<div class="contact-container"> 
							<div class="row m-b-10" style="margin-bottom: 4%;">
								<div class="col-lg-12 text-center">
									 <div class="navy-line"></div> 
									 <h1 style="font-size: 150%;">Team</h1>
							</div>
							</div>
							<div class="row justify-content-center">
								<div class="col-sm-2">
									<div class="team-member">
										<h4><span class="navy">Ernesto</span> Bocini</h4>
										<p>______</p>
										<ul class="list-inline social-icon"> 
											<li><a href="https://github.com/ernestoBocini" target="blank"><i class="icon brands fa-github"></i></a> </li>
											<li><a href="mailto:ernesto.bocini@epfl.ch"><i class="fa fa-envelope"></i></a></li>
										</ul>
									</div>
								</div>
								<div class="col-sm-2">
									<div class="team-member">
										<h4><span class="navy">Lorenzo</span> Drudi</h4>
										<p>______</p>
										<ul class="list-inline social-icon">
											<li><a href="https://github.com/drudilorenzo" target="blank"><i class="icon brands fa-github"></i></a> </li>
											<li><a href="mailto:lorenzo.drudi@epfl.ch"><i class="fa fa-envelope"></i></a></li>
										</ul>
									</div>
								</div>
								<div class="col-sm-2">
									<div class="team-member">
										<h4><span class="navy">Kaede</span> Johnson</h4>
										<p>______</p>
										<ul class="list-inline social-icon">
											<li><a href="https://github.com/kaedejohnson" target="blank"><i class="icon brands fa-github"></i></a> </li>
											<li><a href="mailto:kaede.johnson@epfl.ch"><i class="fa fa-envelope"></i></a> </li>
										 </ul>
									</div>
								</div>
								<div class="col-sm-2">
									<div class="team-member">
										<h4><span class="navy">Hanwen</span> Zhang</h4>
										<p>______</p>
										<ul class="list-inline social-icon">
											<li><a href="https://github.com/Katie-zhang" target="blank"><i class="icon brands fa-github"></i></a> </li>
											<li><a href="mailto:hanwen.zhang@epfl.ch"><i class="fa fa-envelope"></i></a> </li>
										</ul>
									</div>
								</div>
								<div class="col-sm-2">
									<div class="team-member">
										<h4><span class="navy">Xingyue</span> Zhang</h4>
										<p>______</p>
										<ul class="list-inline social-icon">
											<li><a href="https://github.com/bREAKtHEdOLL" target="blank"><i class="icon brands fa-github"></i></a> </li>
											<li><a href="mailto:Xingyue.zhang@epfl.ch"><i class="fa fa-envelope"></i></a> </li>
										</ul>
									</div>
								</div>
							</div>

					<ul id="repo-link" class="menu">
						<li><a href="https://github.com/epfl-ada/ada-2023-project-klech" class="icon brands fa-github"><span class="label">Github</span></a></li>
						<li >&copy; KlechxADA</li></li>
					</ul>

			</footer>

			<!-- Scripts -->
			<script src="assets/js/jquery.min.js"></script>
			<script src="assets/js/jquery.poptrox.min.js"></script>
			<script src="assets/js/jquery.scrolly.min.js"></script>
			<script src="assets/js/jquery.scrollex.min.js"></script>
			<script src="assets/js/browser.min.js"></script>
			<script src="assets/js/breakpoints.min.js"></script>
			<script src="assets/js/util.js"></script>
			<script src="assets/js/main.js"></script>
			<script src="assets/js/typing.js"></script>

			<script src="https://code.jquery.com/jquery-3.2.1.slim.min.js" integrity="sha384-KJ3o2DKtIkvYIK3UENzmM7KCkRr/rE9/Qpg6aAZGJwFDMVNA/GpGFF93hXpG5KkN" crossorigin="anonymous"></script>
			<script src="https://cdn.jsdelivr.net/npm/popper.js@1.12.9/dist/umd/popper.min.js" integrity="sha384-ApNbgh9B+Y1QKtv3Rn7W3mgPxhU9K/ScQsAP7hUibX39j7fakFPskvXusvfa0b4Q" crossorigin="anonymous"></script>
			<script src="https://cdn.jsdelivr.net/npm/bootstrap@4.0.0/dist/js/bootstrap.min.js" integrity="sha384-JZR6Spejh4U02d8jOt6vLEHfe/JQGiRRSQQxSfFWpi1MquVdAyjUar5+76PVCmYl" crossorigin="anonymous"></script>
	</body>
</html>